{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————————下载第5页————————————————————\n",
      "下载第121张\n",
      "下载第122张\n",
      "下载第123张\n",
      "下载第124张\n",
      "下载第125张\n",
      "下载第126张\n",
      "下载第127张\n",
      "下载第128张\n",
      "下载第129张\n",
      "下载第130张\n",
      "下载第131张\n",
      "下载第132张\n",
      "下载第133张\n",
      "下载第134张\n",
      "下载第135张\n",
      "下载第136张\n",
      "下载第137张\n",
      "下载第138张\n",
      "下载第139张\n",
      "下载第140张\n",
      "下载第141张\n",
      "下载第142张\n",
      "下载第143张\n",
      "下载第144张\n",
      "下载第145张\n",
      "下载第146张\n",
      "下载第147张\n",
      "下载第148张\n",
      "下载第149张\n",
      "下载第150张\n",
      "下载第151张\n",
      "——————————下载第6页————————————————————\n",
      "下载第152张\n",
      "下载第153张\n",
      "下载第154张\n",
      "下载第155张\n",
      "下载第156张\n",
      "下载第157张\n",
      "下载第158张\n",
      "下载第159张\n",
      "下载第160张\n",
      "下载第161张\n",
      "下载第162张\n",
      "下载第163张\n",
      "下载第164张\n",
      "下载第165张\n",
      "下载第166张\n",
      "下载第167张\n",
      "下载第168张\n",
      "下载第169张\n",
      "下载第170张\n",
      "下载第171张\n",
      "下载第172张\n",
      "下载第173张\n",
      "下载第174张\n",
      "下载第175张\n",
      "下载第176张\n",
      "下载第177张\n",
      "下载第178张\n",
      "下载第179张\n",
      "下载第180张\n",
      "下载第181张\n",
      "——————————下载第7页————————————————————\n",
      "下载第182张\n",
      "下载第183张\n",
      "下载第184张\n",
      "下载第185张\n",
      "下载第186张\n",
      "下载第187张\n",
      "下载第188张\n",
      "下载第189张\n",
      "下载第190张\n",
      "下载第191张\n",
      "下载第192张\n",
      "下载第193张\n",
      "下载第194张\n",
      "下载第195张\n",
      "下载第196张\n",
      "下载第197张\n",
      "下载第198张\n",
      "下载第199张\n",
      "下载第200张\n",
      "下载第201张\n",
      "下载第202张\n",
      "下载第203张\n",
      "下载第204张\n",
      "下载第205张\n",
      "下载第206张\n",
      "下载第207张\n",
      "下载第208张\n",
      "下载第209张\n",
      "下载第210张\n",
      "——————————下载第8页————————————————————\n",
      "下载第211张\n",
      "下载第212张\n",
      "下载第213张\n",
      "下载第214张\n",
      "**下载完成!**\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getPictureImg():\n",
    "    x = 1\n",
    "    for i in range(5, 9):\n",
    "        url = requests.get(\"https://lyrlxl6.lofter.com/?page=\"+str(i))\n",
    "        #获取网站数据\n",
    "        html = url.text\n",
    "        #解析网页\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        #获取所有的img标签\n",
    "        picture = soup.find_all('img')\n",
    "        print('——————————下载第%d页————————————————————' % i)\n",
    "\n",
    "        for picture in picture:\n",
    "            # 获取src路径\n",
    "            imgsrc = picture.get('src')\n",
    "            # 判断图片src路径是否以指定内容开头（过滤页面中的其它不想要的图片）\n",
    "            if imgsrc.startswith('https://imglf'):\n",
    "                # print(imgsrc)\n",
    "                # 本地路径\n",
    "\n",
    "                filename = 'D:/python爬虫网上数据/lofter/lyrlxl6/%s.jpg' % x\n",
    "                # 将URL表示的网络对象复制到本地文件\n",
    "                urllib.request.urlretrieve(imgsrc, filename)\n",
    "                print('下载第%d张' % x)\n",
    "                x = x + 1\n",
    "\n",
    "\n",
    "\n",
    "getPictureImg()\n",
    "print('**下载完成!**')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
